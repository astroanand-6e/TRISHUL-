{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57b13df-9a4b-4b4d-b1be-067de63325d4",
   "metadata": {},
   "source": [
    " # **\"How do attention mechanisms in smaller language models (Gemma2 2B and LLaMa 3.1 1B) process semantic equivalence across English, Hindi, and Hinglish, and what specific Sparse Autoencoder (SAE) features activate during code-switching that might explain performance differences in multilingual contexts?\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7fa8004-ac65-492e-9920-5c1192e5c6d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:49:16.422623Z",
     "iopub.status.busy": "2025-02-27T09:49:16.421892Z",
     "iopub.status.idle": "2025-02-27T09:49:16.424744Z",
     "shell.execute_reply": "2025-02-27T09:49:16.424402Z",
     "shell.execute_reply.started": "2025-02-27T09:49:16.422601Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install 'accelerate>=0.26.0'\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a98521-291c-4e74-9e2a-481555ab1b98",
   "metadata": {},
   "source": [
    "## Loading Gemma2 2b model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3764a826-4488-4dca-ac48-3d30cbd8bd43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:49:18.585507Z",
     "iopub.status.busy": "2025-02-27T09:49:18.584849Z",
     "iopub.status.idle": "2025-02-27T09:49:18.588564Z",
     "shell.execute_reply": "2025-02-27T09:49:18.588218Z",
     "shell.execute_reply.started": "2025-02-27T09:49:18.585487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd47305b-4e1f-4937-a8d8-6b2aab3bd4bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:49:19.734756Z",
     "iopub.status.busy": "2025-02-27T09:49:19.734143Z",
     "iopub.status.idle": "2025-02-27T09:49:22.095996Z",
     "shell.execute_reply": "2025-02-27T09:49:22.095270Z",
     "shell.execute_reply.started": "2025-02-27T09:49:19.734756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as: astroanand\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Hardcode Hugging Face token\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Log in to Hugging Face Hub\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Verify login (optional)\n",
    "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d56eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:49:22.099015Z",
     "iopub.status.busy": "2025-02-27T09:49:22.098688Z",
     "iopub.status.idle": "2025-02-27T09:49:24.699504Z",
     "shell.execute_reply": "2025-02-27T09:49:24.698943Z",
     "shell.execute_reply.started": "2025-02-27T09:49:22.098991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 15379.12 MB free out of 16108.75 MB total\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set memory configuration to avoid fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Completely disable torch dynamo to avoid compatibility issues\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "# Define your device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**2:.2f} MB free out of {torch.cuda.mem_get_info()[1] / 1024**2:.2f} MB total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e045b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:49:29.051616Z",
     "iopub.status.busy": "2025-02-27T09:49:29.051162Z",
     "iopub.status.idle": "2025-02-27T09:49:37.404471Z",
     "shell.execute_reply": "2025-02-27T09:49:37.403859Z",
     "shell.execute_reply.started": "2025-02-27T09:49:29.051597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gemma2 2B in FP16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 09:49:30.441798: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-27 09:49:30.484890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-27 09:49:30.484940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-27 09:49:30.485883: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-27 09:49:30.491295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-27 09:49:31.191543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3f630cf7e5496d8d7ae9a4147cb015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2 2B loaded in FP16!\n",
      "Gemma2-2B has 26 layers with 8 query heads and 4 key/value heads per layer.\n",
      "\n",
      "Extracted attention shapes for all layers and heads:\n",
      "Layer 0: 8 heads, sequence length 13x13\n",
      "Layer 1: 8 heads, sequence length 13x13\n",
      "Layer 2: 8 heads, sequence length 13x13\n",
      "Layer 3: 8 heads, sequence length 13x13\n",
      "Layer 4: 8 heads, sequence length 13x13\n",
      "Layer 5: 8 heads, sequence length 13x13\n",
      "Layer 6: 8 heads, sequence length 13x13\n",
      "Layer 7: 8 heads, sequence length 13x13\n",
      "Layer 8: 8 heads, sequence length 13x13\n",
      "Layer 9: 8 heads, sequence length 13x13\n",
      "Layer 10: 8 heads, sequence length 13x13\n",
      "Layer 11: 8 heads, sequence length 13x13\n",
      "Layer 12: 8 heads, sequence length 13x13\n",
      "Layer 13: 8 heads, sequence length 13x13\n",
      "Layer 14: 8 heads, sequence length 13x13\n",
      "Layer 15: 8 heads, sequence length 13x13\n",
      "Layer 16: 8 heads, sequence length 13x13\n",
      "Layer 17: 8 heads, sequence length 13x13\n",
      "Layer 18: 8 heads, sequence length 13x13\n",
      "Layer 19: 8 heads, sequence length 13x13\n",
      "Layer 20: 8 heads, sequence length 13x13\n",
      "Layer 21: 8 heads, sequence length 13x13\n",
      "Layer 22: 8 heads, sequence length 13x13\n",
      "Layer 23: 8 heads, sequence length 13x13\n",
      "Layer 24: 8 heads, sequence length 13x13\n",
      "Layer 25: 8 heads, sequence length 13x13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to load model and tokenizer in FP16\n",
    "def load_model_and_tokenizer(model_name, token=HF_TOKEN):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=token,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=True  # Ensure fast tokenizer for better performance\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=token,\n",
    "        device_map=\"auto\",  # Automatically map to GPU\n",
    "        torch_dtype=torch.float16,  # Use FP16 for efficiency\n",
    "        low_cpu_mem_usage=True,  # Reduce CPU memory usage\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\"  # Ensure compatibility\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load Gemma2 2B in FP16\n",
    "print(\"Loading Gemma2 2B in FP16...\")\n",
    "tokenizer_2b, model_2b = load_model_and_tokenizer(\"google/gemma-2-2b\")\n",
    "print(\"Gemma2 2B loaded in FP16!\")\n",
    "\n",
    "# Move model to GPU explicitly (if not handled by device_map)\n",
    "model_2b.to(device)\n",
    "\n",
    "# Function to inspect model architecture (layers and heads)\n",
    "def inspect_model_architecture(model):\n",
    "    num_layers = len(model.model.layers)  # Number of transformer layers\n",
    "    \n",
    "    # For Gemma 2, we need to access the attention configuration differently\n",
    "    # Get the head dimension\n",
    "    head_dim = model.model.layers[0].self_attn.head_dim\n",
    "    \n",
    "    # Calculate number of query heads\n",
    "    q_heads = model.model.layers[0].self_attn.q_proj.out_features // head_dim\n",
    "    \n",
    "    # Calculate number of key/value heads (for GQA, this is fewer than q_heads)\n",
    "    kv_heads = model.model.layers[0].self_attn.k_proj.out_features // head_dim\n",
    "    \n",
    "    return num_layers, q_heads, kv_heads\n",
    "\n",
    "# Check model architecture\n",
    "num_layers, q_heads, kv_heads = inspect_model_architecture(model_2b)\n",
    "print(f\"Gemma2-2B has {num_layers} layers with {q_heads} query heads and {kv_heads} key/value heads per layer.\")\n",
    "\n",
    "# Sample prompt for analysis\n",
    "sample_prompt = \"Will you please help me understand the concept of kinetic energy?\"\n",
    "inputs_2b = tokenizer_2b(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Run a forward pass to capture attention on the inputs\n",
    "with torch.no_grad():\n",
    "    outputs = model_2b(**inputs_2b, output_attentions=True, return_dict=True)\n",
    "\n",
    "# outputs.attentions is a tuple with one element per layer\n",
    "attentions = outputs.attentions\n",
    "\n",
    "# Print attention shapes for all layers and heads\n",
    "print(\"\\nExtracted attention shapes for all layers and heads:\")\n",
    "for layer_idx, attn in enumerate(attentions):\n",
    "    batch_size, num_heads, seq_length, _ = attn.shape\n",
    "    print(f\"Layer {layer_idx}: {num_heads} heads, sequence length {seq_length}x{seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540afea6-7210-422b-acc6-d661f5f9c522",
   "metadata": {},
   "source": [
    "### Trilingual prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b385ab-fc30-4295-a92b-2aa657f0d6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:49:56.001057Z",
     "iopub.status.busy": "2025-02-27T09:49:56.000860Z",
     "iopub.status.idle": "2025-02-27T09:49:56.004715Z",
     "shell.execute_reply": "2025-02-27T09:49:56.004139Z",
     "shell.execute_reply.started": "2025-02-27T09:49:56.001039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define trilingual prompts for later use\n",
    "prompts = {\n",
    "    \"trio1\": {\n",
    "        \"english\": \"Will you please help me understand the concept of kinetic energy?\",\n",
    "        \"hindi\": \"क्या आप कृपया मुझे गतिज ऊर्जा की अवधारणा को समझने में मदद करेंगे?\",\n",
    "        \"hinglish\": \"Kya aap mujhe kinetic energy ke concept ko samajhne mein help karoge?\"\n",
    "    },\n",
    "    \"trio2\": {\n",
    "        \"english\": \"I want you to tell me a secret about the stars tonight.\",\n",
    "        \"hindi\": \"मैं चाहता हूँ कि आप आज रात मुझे सितारों के बारे में एक रहस्य बताएँ।\",\n",
    "        \"hinglish\": \"Main chahta hoon ki aaj raat aap mujhe stars ke baare mein ek secret batao.\"\n",
    "    },\n",
    "    \"trio3\": {\n",
    "        \"english\": \"I understand kinetic energy.\",\n",
    "        \"hindi\": \"मुझे काइनेटिक ऊर्जा समझ आती है।\",\n",
    "        \"hinglish\": \"Mujhe kinetic energy samajh aata hai.\"\n",
    "    },\n",
    "    \"trio4\": {\n",
    "        \"english\": \"Can you help me learn about gravity?\",\n",
    "        \"hindi\": \"क्या आप मुझे गुरुत्वाकर्षण के बारे में सिखा सकते हैं?\",\n",
    "        \"hinglish\": \"Kya aap mujhe gravity ke bare mein sikha sakte hain?\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f8240-a926-429c-bd6a-25c71423ef9a",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4662b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:49:58.297795Z",
     "iopub.status.busy": "2025-02-27T09:49:58.297551Z",
     "iopub.status.idle": "2025-02-27T09:49:58.521561Z",
     "shell.execute_reply": "2025-02-27T09:49:58.520789Z",
     "shell.execute_reply.started": "2025-02-27T09:49:58.297779Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1190e25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:50:32.200871Z",
     "iopub.status.busy": "2025-02-27T09:50:32.200411Z",
     "iopub.status.idle": "2025-02-27T09:53:50.770583Z",
     "shell.execute_reply": "2025-02-27T09:53:50.770074Z",
     "shell.execute_reply.started": "2025-02-27T09:50:32.200856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and storing attention patterns and model outputs...\n",
      "Attention data saved to attention_data_gemma2.pkl\n",
      "Model outputs saved to output_data_gemma2.pkl\n",
      "Data extraction and storage complete.\n"
     ]
    }
   ],
   "source": [
    "# Function to extract attention for all layers and heads and store model outputs\n",
    "def extract_and_store_data(model, tokenizer, prompts, attention_path=\"attention_data_gemma2.pkl\", output_path=\"output_data_gemma2.pkl\"):\n",
    "    model.eval()\n",
    "    attention_dict = {}\n",
    "    output_dict = {}\n",
    "    \n",
    "    for trio_name, languages in prompts.items():\n",
    "        attention_dict[trio_name] = {}\n",
    "        output_dict[trio_name] = {}\n",
    "        \n",
    "        for lang, prompt in languages.items():\n",
    "            attention_dict[trio_name][lang] = {}\n",
    "            \n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Store the original prompt\n",
    "            attention_dict[trio_name][lang][\"prompt\"] = prompt\n",
    "            \n",
    "            # Get token IDs and decode them to get individual tokens\n",
    "            token_ids = inputs.input_ids[0].cpu().numpy()\n",
    "            tokens = [tokenizer.decode([token_id], skip_special_tokens=True) for token_id in token_ids]\n",
    "            \n",
    "            # Store the tokens\n",
    "            attention_dict[trio_name][lang][\"tokens\"] = tokens\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_attentions=True, return_dict=True)\n",
    "                attentions = outputs.attentions  # Tuple of attention tensors, one per layer\n",
    "                \n",
    "                # Generate model's response for the prompt\n",
    "                generation_output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=512,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                model_response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Store the model's response in output_dict\n",
    "                output_dict[trio_name][lang] = model_response\n",
    "            \n",
    "            # Create a sub-dictionary for attention matrices\n",
    "            attention_dict[trio_name][lang][\"attention\"] = {}\n",
    "            \n",
    "            for layer_idx in range(len(attentions)):  # 0 to 25\n",
    "                attention_dict[trio_name][lang][\"attention\"][layer_idx] = {}\n",
    "                layer_attention = attentions[layer_idx].squeeze(0)  # Shape: (num_heads, seq_len, seq_len)\n",
    "                for head_idx in range(layer_attention.shape[0]):  # 0 to 7\n",
    "                    attention_dict[trio_name][lang][\"attention\"][layer_idx][head_idx] = layer_attention[head_idx].cpu().numpy()\n",
    "    \n",
    "    # Save attention data to pickle file\n",
    "    with open(attention_path, 'wb') as f:\n",
    "        pickle.dump(attention_dict, f)\n",
    "    print(f\"Attention data saved to {attention_path}\")\n",
    "    \n",
    "    # Save model outputs to pickle file\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(output_dict, f)\n",
    "    print(f\"Model outputs saved to {output_path}\")\n",
    "\n",
    "    return attention_dict, output_dict\n",
    "\n",
    "\n",
    "# Extract and store attention patterns and model outputs\n",
    "print(\"Extracting and storing attention patterns and model outputs...\")\n",
    "attention_patterns, model_outputs = extract_and_store_data(\n",
    "    model_2b, \n",
    "    tokenizer_2b, \n",
    "    prompts, \n",
    "    attention_path=\"attention_data_gemma2.pkl\", \n",
    "    output_path=\"output_data_gemma2.pkl\"\n",
    ")\n",
    "print(\"Data extraction and storage complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fddb9-c5fe-492b-8c87-08e4319ae785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3791dac4-7827-4ec8-976c-ffdfe835d9d2",
   "metadata": {},
   "source": [
    " # **\"How do attention mechanisms in smaller language models (Gemma2 2B and LLaMa 3.1 1B) process semantic equivalence across English, Hindi, and Hinglish, and what specific Sparse Autoencoder (SAE) features activate during code-switching that might explain performance differences in multilingual contexts?\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7dd084-27dd-4859-89df-798ebf59962d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T10:06:33.323036Z",
     "iopub.status.busy": "2025-02-27T10:06:33.322503Z",
     "iopub.status.idle": "2025-02-27T10:06:33.326295Z",
     "shell.execute_reply": "2025-02-27T10:06:33.325698Z",
     "shell.execute_reply.started": "2025-02-27T10:06:33.323008Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install 'accelerate>=0.26.0'\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b9a3c9-7372-4a31-b7e3-8d0a3e6eb916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T10:06:33.328014Z",
     "iopub.status.busy": "2025-02-27T10:06:33.327809Z",
     "iopub.status.idle": "2025-02-27T10:06:33.334755Z",
     "shell.execute_reply": "2025-02-27T10:06:33.334085Z",
     "shell.execute_reply.started": "2025-02-27T10:06:33.327996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfc6510-7698-4c17-a37a-9da3e433d3ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T10:06:33.335564Z",
     "iopub.status.busy": "2025-02-27T10:06:33.335414Z",
     "iopub.status.idle": "2025-02-27T10:06:35.954185Z",
     "shell.execute_reply": "2025-02-27T10:06:35.953592Z",
     "shell.execute_reply.started": "2025-02-27T10:06:33.335549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as:  astroanand\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Hardcoding HF_Token because IDK how to use Secrets in paperspace Gradient\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(\"Logged in as: \", whoami(token=HF_TOKEN)['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216fb92c-479f-41c7-b91b-dc784799d21e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T10:06:35.955447Z",
     "iopub.status.busy": "2025-02-27T10:06:35.955191Z",
     "iopub.status.idle": "2025-02-27T10:06:38.542888Z",
     "shell.execute_reply": "2025-02-27T10:06:38.542431Z",
     "shell.execute_reply.started": "2025-02-27T10:06:35.955430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 8965.12 MB free out of 16108.75 MB total\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set memory configuration to avoid fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Completely disable torch dynamo to avoid compatibility issues\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "# Define your device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**2:.2f} MB free out of {torch.cuda.mem_get_info()[1] / 1024**2:.2f} MB total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a292ee-5dd0-4a8c-a1fe-5190e52f1640",
   "metadata": {},
   "source": [
    "### Loading Llama3.1 1B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c0fe19-8900-4304-8c93-c564695a0078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T10:06:38.544595Z",
     "iopub.status.busy": "2025-02-27T10:06:38.544117Z",
     "iopub.status.idle": "2025-02-27T10:06:44.311372Z",
     "shell.execute_reply": "2025-02-27T10:06:44.310832Z",
     "shell.execute_reply.started": "2025-02-27T10:06:38.544578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up device...\n",
      "Using device: cuda\n",
      "Loading Llama3.2 1B model...\n",
      "Tokenizer loaded successfully\n",
      "Model loaded successfully\n",
      "\n",
      "Llama3.2-1B has 16 layers\n",
      "Query heads: 32, Key/Value heads: 8\n",
      "\n",
      "Extracted attention shapes for all layers and heads:\n",
      "Layer 0: 32 heads, sequence length 9x9\n",
      "Layer 1: 32 heads, sequence length 9x9\n",
      "Layer 2: 32 heads, sequence length 9x9\n",
      "Layer 3: 32 heads, sequence length 9x9\n",
      "Layer 4: 32 heads, sequence length 9x9\n",
      "Layer 5: 32 heads, sequence length 9x9\n",
      "Layer 6: 32 heads, sequence length 9x9\n",
      "Layer 7: 32 heads, sequence length 9x9\n",
      "Layer 8: 32 heads, sequence length 9x9\n",
      "Layer 9: 32 heads, sequence length 9x9\n",
      "Layer 10: 32 heads, sequence length 9x9\n",
      "Layer 11: 32 heads, sequence length 9x9\n",
      "Layer 12: 32 heads, sequence length 9x9\n",
      "Layer 13: 32 heads, sequence length 9x9\n",
      "Layer 14: 32 heads, sequence length 9x9\n",
      "Layer 15: 32 heads, sequence length 9x9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Completely remove TensorFlow from sys.modules before importing transformers\n",
    "for module in list(sys.modules.keys()):\n",
    "    if 'tensorflow' in module or 'tf_' in module or module == 'tf':\n",
    "        sys.modules.pop(module, None)\n",
    "\n",
    "# Set environment variables to prevent TensorFlow loading\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Import AutoTokenizer instead of LlamaTokenizer\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "print(\"Setting up device...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading Llama3.2 1B model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load tokenizer using AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True\n",
    ")\n",
    "print(\"Tokenizer loaded successfully\")\n",
    "\n",
    "# Load model with minimal options\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,  # Reduce CPU memory usage\n",
    "    #trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"    \n",
    ")\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Count layers\n",
    "num_layers = len(model.model.layers)\n",
    "print(f\"\\nLlama3.2-1B has {num_layers} layers\")\n",
    "\n",
    "# Get head dimensions from first layer\n",
    "head_dim = model.model.layers[0].self_attn.head_dim\n",
    "q_heads = model.model.layers[0].self_attn.q_proj.out_features // head_dim\n",
    "kv_heads = model.model.layers[0].self_attn.k_proj.out_features // head_dim\n",
    "print(f\"Query heads: {q_heads}, Key/Value heads: {kv_heads}\")\n",
    "\n",
    "# Sample text for attention analysis\n",
    "text = \"Explain the concept of kinetic energy.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate with attention outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Print attention shapes\n",
    "if outputs.attentions:\n",
    "    print(\"\\nExtracted attention shapes for all layers and heads:\")\n",
    "    for layer_idx, attn in enumerate(outputs.attentions):\n",
    "        batch_size, num_heads, seq_length, _ = attn.shape\n",
    "        print(f\"Layer {layer_idx}: {num_heads} heads, sequence length {seq_length}x{seq_length}\")\n",
    "else:\n",
    "    print(\"\\nNo attention outputs were returned.\")\n",
    "    print(\"Manually inspecting attention configuration:\")\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        head_dim = layer.self_attn.head_dim\n",
    "        q_heads = layer.self_attn.q_proj.out_features // head_dim\n",
    "        kv_heads = layer.self_attn.k_proj.out_features // head_dim\n",
    "        print(f\"Layer {layer_idx}: {kv_heads} KV heads (GQA with {q_heads} query heads)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a176a140-34ae-4e68-8e18-b4511506176e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T10:06:44.312925Z",
     "iopub.status.busy": "2025-02-27T10:06:44.312177Z",
     "iopub.status.idle": "2025-02-27T10:06:44.316327Z",
     "shell.execute_reply": "2025-02-27T10:06:44.315833Z",
     "shell.execute_reply.started": "2025-02-27T10:06:44.312898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define trilingual prompts for later use\n",
    "prompts = {\n",
    "    \"trio1\": {\n",
    "        \"english\": \"Will you please help me understand the concept of kinetic energy?\",\n",
    "        \"hindi\": \"क्या आप कृपया मुझे गतिज ऊर्जा की अवधारणा को समझने में मदद करेंगे?\",\n",
    "        \"hinglish\": \"Kya aap mujhe kinetic energy ke concept ko samajhne mein help karoge?\"\n",
    "    },\n",
    "    \"trio2\": {\n",
    "        \"english\": \"I want you to tell me a secret about the stars tonight.\",\n",
    "        \"hindi\": \"मैं चाहता हूँ कि आप आज रात मुझे सितारों के बारे में एक रहस्य बताएँ।\",\n",
    "        \"hinglish\": \"Main chahta hoon ki aaj raat aap mujhe stars ke baare mein ek secret batao.\"\n",
    "    },\n",
    "    \"trio3\": {\n",
    "        \"english\": \"I understand kinetic energy.\",\n",
    "        \"hindi\": \"मुझे काइनेटिक ऊर्जा समझ आती है।\",\n",
    "        \"hinglish\": \"Mujhe kinetic energy samajh aata hai.\"\n",
    "    },\n",
    "    \"trio4\": {\n",
    "        \"english\": \"Can you help me learn about gravity?\",\n",
    "        \"hindi\": \"क्या आप मुझे गुरुत्वाकर्षण के बारे में सिखा सकते हैं?\",\n",
    "        \"hinglish\": \"Kya aap mujhe gravity ke bare mein sikha sakte hain?\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3318198a-9b37-4143-a175-7b13cbded683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T10:06:44.317123Z",
     "iopub.status.busy": "2025-02-27T10:06:44.316955Z",
     "iopub.status.idle": "2025-02-27T10:08:14.650271Z",
     "shell.execute_reply": "2025-02-27T10:08:14.649552Z",
     "shell.execute_reply.started": "2025-02-27T10:06:44.317108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and storing attention patterns and model outputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention data saved to attention_data_Llama3.2.pkl\n",
      "Model outputs saved to output_data_Llama3.2.pkl\n",
      "Data extraction and storage complete.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Function to extract attention for all layers and heads and store model outputs\n",
    "def extract_and_store_data(model, tokenizer, prompts, attention_path=\"attention_data_Llama3.2.pkl\", output_path=\"output_data_Llama3.2.pkl\"):\n",
    "    model.eval()\n",
    "    attention_dict = {}\n",
    "    output_dict = {}\n",
    "    \n",
    "    for trio_name, languages in prompts.items():\n",
    "        attention_dict[trio_name] = {}\n",
    "        output_dict[trio_name] = {}\n",
    "        \n",
    "        for lang, prompt in languages.items():\n",
    "            attention_dict[trio_name][lang] = {}\n",
    "            \n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Store the original prompt\n",
    "            attention_dict[trio_name][lang][\"prompt\"] = prompt\n",
    "            \n",
    "            # Get token IDs and decode them to get individual tokens\n",
    "            token_ids = inputs.input_ids[0].cpu().numpy()\n",
    "            tokens = [tokenizer.decode([token_id], skip_special_tokens=True) for token_id in token_ids]\n",
    "            \n",
    "            # Store the tokens\n",
    "            attention_dict[trio_name][lang][\"tokens\"] = tokens\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_attentions=True, return_dict=True)\n",
    "                attentions = outputs.attentions  # Tuple of attention tensors, one per layer\n",
    "                \n",
    "                # Generate model's response for the prompt\n",
    "                generation_output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=512,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                model_response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Store the model's response in output_dict\n",
    "                output_dict[trio_name][lang] = model_response\n",
    "            \n",
    "            # Create a sub-dictionary for attention matrices\n",
    "            attention_dict[trio_name][lang][\"attention\"] = {}\n",
    "            \n",
    "            for layer_idx in range(len(attentions)):  # Iterate through all layers\n",
    "                attention_dict[trio_name][lang][\"attention\"][layer_idx] = {}\n",
    "                layer_attention = attentions[layer_idx].squeeze(0)  # Shape: (num_heads, seq_len, seq_len)\n",
    "                for head_idx in range(layer_attention.shape[0]):  # Iterate through all heads\n",
    "                    attention_dict[trio_name][lang][\"attention\"][layer_idx][head_idx] = layer_attention[head_idx].cpu().numpy()\n",
    "    \n",
    "    # Save attention data to pickle file\n",
    "    with open(attention_path, 'wb') as f:\n",
    "        pickle.dump(attention_dict, f)\n",
    "    print(f\"Attention data saved to {attention_path}\")\n",
    "    \n",
    "    # Save model outputs to pickle file\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(output_dict, f)\n",
    "    print(f\"Model outputs saved to {output_path}\")\n",
    "\n",
    "    return attention_dict, output_dict\n",
    "\n",
    "\n",
    "# Extract and store attention patterns and model outputs\n",
    "print(\"Extracting and storing attention patterns and model outputs...\")\n",
    "attention_patterns, model_outputs = extract_and_store_data(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompts, \n",
    "    attention_path=\"attention_data_Llama3.2.pkl\", \n",
    "    output_path=\"output_data_Llama3.2.pkl\"\n",
    ")\n",
    "print(\"Data extraction and storage complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
